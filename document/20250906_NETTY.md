Netty는 유지 보수가 가능한 고성능 프로토콜 서버 및 클라이언트의 빠른 개발을 위한 비동기 이벤트 기반 네트워크 애플리케이션 프레임워크 입니다.

Netty는 프로토콜 서버 및 클라이언트와 같은 네트워크 애플리케이션을 빠르고 쉽게 개발할 수 있는 NIO 클라이언트 서버 프레임워크입니다. TCP 및 UDP 소켓 서버와 같은 네트워크 프로그래밍을 크게 간소화하고 효율화합니다.

'빠르고 쉽다'는 것이 결과적으로 애플리케이션이 유지 관리나 성능 문제를 겪게 된다는 것을 의미하지는 않습니다. Netty는 FTP, SMTP, HTTP, 그리고 다양한 바이너리 및 텍스트 기반 레거시 프로토콜을 구현하면서 얻은 경험을 바탕으로 신중하게 설계되었습니다. 그 결과, Netty는 개발 편의성, 성능, 안정성, 그리고 유연성을 타협 없이 달성하는 방법을 찾아냈습니다.

특징
설계
다양한 전송 유형(차단 및 비차단 소켓)에 대한 통합 API
관심사를 명확하게 분리할 수 있는 유연하고 확장 가능한 이벤트 모델을 기반으로 합니다.
고도로 사용자 정의 가능한 스레드 모델 - 단일 스레드, SEDA와 같은 하나 이상의 스레드 풀
진정한 비연결형 데이터그램 소켓 지원(3.1부터)
사용 편의성
잘 문서화된 Javadoc, 사용자 가이드 및 예제
추가 종속성 없음, JDK 5(Netty 3.x) 또는 6(Netty 4.x)이면 충분합니다.
참고: HTTP/2와 같은 일부 구성 요소에는 추가 요구 사항이 있을 수 있습니다. 자세한 내용은 요구 사항 페이지 를 참조하세요.
성능
더 나은 처리량, 더 낮은 지연 시간
자원 소모 감소
불필요한 메모리 복사 최소화
보안
완벽한 SSL/TLS 및 StartTLS 지원
지역 사회
일찍 출시하고 자주 출시하세요
저자는 2003년부터 비슷한 프레임워크를 작성해왔고, 여전히 여러분의 피드백이 소중하다고 생각합니다!

---
4.x 사용자 가이드
이 페이지가 Github Wiki 페이지 에서 자동 생성된다는 사실, 알고 계셨나요 ? 여기에서 직접 개선할 수 있습니다 !
머리말
문제
요즘 우리는 서로 통신하기 위해 범용 애플리케이션이나 라이브러리를 사용합니다. 예를 들어, 웹 서버에서 정보를 검색하고 웹 서비스를 통해 원격 프로시저 호출(RPC)을 호출하기 위해 HTTP 클라이언트 라이브러리를 사용하는 경우가 많습니다. 그러나 범용 프로토콜이나 그 구현은 확장성이 떨어지는 경우가 있습니다. 마치 범용 HTTP 서버를 사용하여 대용량 파일, 이메일 메시지, 금융 정보나 멀티플레이어 게임 데이터와 같은 실시간에 가까운 메시지를 교환하지 않는 것과 같습니다. 필요한 것은 특정 목적에 맞춰 고도로 최적화된 프로토콜 구현입니다. 예를 들어 AJAX 기반 채팅 애플리케이션, 미디어 스트리밍 또는 대용량 파일 전송에 최적화된 HTTP 서버를 구현할 수 있습니다. 심지어 필요에 맞춰 완전히 새로운 프로토콜을 설계하고 구현할 수도 있습니다. 또 다른 불가피한 경우는 기존 시스템과의 상호 운용성을 보장하기 위해 기존 독점 프로토콜을 사용해야 하는 경우입니다. 이 경우 중요한 것은 결과 애플리케이션의 안정성과 성능을 저하시키지 않으면서 해당 프로토콜을 얼마나 빨리 구현할 수 있느냐입니다.

해결책
Netty 프로젝트는 유지 보수가 가능한 고성능, 고확장성 프로토콜 서버와 클라이언트를 빠르게 개발할 수 있는 비동기 이벤트 기반 네트워크 애플리케이션 프레임워크와 도구를 제공하기 위한 노력입니다.

즉, Netty는 프로토콜 서버 및 클라이언트와 같은 네트워크 애플리케이션을 빠르고 쉽게 개발할 수 있는 NIO 클라이언트 서버 프레임워크입니다. TCP 및 UDP 소켓 서버 개발과 같은 네트워크 프로그래밍을 크게 단순화하고 효율화합니다.

'빠르고 쉽다'는 것이 결과적으로 애플리케이션이 유지 관리나 성능 문제를 겪게 된다는 것을 의미하지는 않습니다. Netty는 FTP, SMTP, HTTP, 그리고 다양한 바이너리 및 텍스트 기반 레거시 프로토콜을 구현하면서 얻은 경험을 바탕으로 신중하게 설계되었습니다. 그 결과, Netty는 개발 편의성, 성능, 안정성, 그리고 유연성을 타협 없이 달성하는 방법을 찾아냈습니다.

일부 사용자는 이미 동일한 장점을 제공한다고 주장하는 다른 네트워크 애플리케이션 프레임워크를 발견했을 수도 있습니다. Netty가 다른 프레임워크와 어떻게 다른지 궁금할 수도 있습니다. 정답은 바로 Netty가 기반으로 하는 철학입니다. Netty는 API와 구현 방식 모두에서 처음부터 가장 편안한 경험을 제공하도록 설계되었습니다. 구체적인 내용은 아니지만, 이 가이드를 읽고 Netty를 사용해 보면 이러한 철학이 여러분의 삶을 훨씬 더 편리하게 만들어 줄 것임을 알게 될 것입니다.

시작하기
이 장에서는 Netty의 핵심 구성 요소를 간단한 예제와 함께 살펴보고 빠르게 시작할 수 있도록 안내합니다. 이 장을 마치면 Netty를 기반으로 클라이언트와 서버를 바로 작성할 수 있게 될 것입니다.

무언가를 위에서 아래로 학습하는 방식을 선호한다면 2장 아키텍처 개요 부터 시작하여 여기로 돌아오는 것이 좋습니다.

시작하기 전에
이 장의 예제를 실행하기 위한 최소 요구 사항은 Netty 최신 버전과 JDK 1.6 이상, 두 가지뿐입니다. Netty 최신 버전은 프로젝트 다운로드 페이지 에서 다운로드할 수 있습니다 . 적합한 JDK 버전을 다운로드하려면 선호하는 JDK 공급업체의 웹사이트를 참조하세요.

이 장에서 소개된 클래스에 대해 더 많은 질문이 있으실 수 있습니다. 더 자세히 알고 싶으시면 API 참조를 참조하세요. 이 문서의 모든 클래스 이름은 편의를 위해 온라인 API 참조에 링크되어 있습니다. 또한, Netty 프로젝트 커뮤니티에 연락하여 잘못된 정보, 문법 오류 또는 오타가 있거나 문서 개선에 도움이 될 만한 좋은 아이디어가 있으면 언제든지 알려주세요.

Discard 서버 작성
세상에서 가장 단순한 프로토콜은 'Hello, World!'가 아니라, 입니다 DISCARD. 수신된 데이터를 아무런 응답 없이 버리는 프로토콜입니다.

프로토콜을 구현하려면 DISCARD수신된 모든 데이터를 무시하기만 하면 됩니다. Netty에서 생성된 I/O 이벤트를 처리하는 핸들러 구현부터 시작해 보겠습니다.

package io.netty.example.discard;

import io.netty.buffer.ByteBuf;

import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;

/**
 * Handles a server-side channel.
 */
public class DiscardServerHandler extends ChannelInboundHandlerAdapter { // (1)

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) { // (2)
        // Discard the received data silently.
        ((ByteBuf) msg).release(); // (3)
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) { // (4)
        // Close the connection when an exception is raised.
        cause.printStackTrace();
        ctx.close();
    }
}
DiscardServerHandlerextends 는 . ChannelInboundHandlerAdapter의 구현체입니다 ChannelInboundHandler. ChannelInboundHandler오버라이드할 수 있는 다양한 이벤트 핸들러 메서드를 제공합니다. 현재로서는 ChannelInboundHandlerAdapter핸들러 인터페이스를 직접 구현하기보다는 확장하는 것만으로도 충분합니다.
여기서 이벤트 핸들러 메서드를 재정의합니다 channelRead(). 이 메서드는 클라이언트로부터 새 데이터를 수신할 때마다 수신된 메시지와 함께 호출됩니다. 이 예제에서 수신된 메시지의 유형은 ByteBuf. 입니다.
프로토콜을 구현하려면 DISCARD핸들러가 수신된 메시지를 무시해야 합니다. ByteBuf는 메서드를 통해 명시적으로 해제해야 하는 참조 카운트 객체입니다 release(). 핸들러에 전달된 참조 카운트 객체를 해제하는 것은 핸들러의 책임이라는 점에 유의하세요. 일반적으로 channelRead()핸들러 메서드는 다음과 같이 구현됩니다.
@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) {
    try {
        // Do something with msg
    } finally {
        ReferenceCountUtil.release(msg);
    }
}
이벤트 exceptionCaught()핸들러 메서드는 Netty에서 I/O 오류로 인해 예외가 발생하거나, 이벤트 처리 중 발생한 예외로 인해 핸들러 구현에서 예외가 발생하면 Throwable과 함께 호출됩니다. 대부분의 경우, 발생한 예외는 로깅되고 관련 채널은 여기서 닫힙니다. 하지만 이 메서드의 구현 방식은 예외 상황을 처리하기 위해 수행하려는 작업에 따라 달라질 수 있습니다. 예를 들어, 연결을 닫기 전에 오류 코드가 포함된 응답 메시지를 전송할 수 있습니다.
지금까지는 잘 됩니다. 서버의 전반부를 구현했습니다 DISCARD. 이제 남은 것은 . main()로 서버를 시작하는 메서드를 작성하는 것입니다 DiscardServerHandler.

package io.netty.example.discard;
    
import io.netty.bootstrap.ServerBootstrap;

import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.ChannelOption;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.SocketChannel;
import io.netty.channel.socket.nio.NioServerSocketChannel;
    
/**
 * Discards any incoming data.
 */
public class DiscardServer {
    
    private int port;
    
    public DiscardServer(int port) {
        this.port = port;
    }
    
    public void run() throws Exception {
        EventLoopGroup bossGroup = new NioEventLoopGroup(); // (1)
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        try {
            ServerBootstrap b = new ServerBootstrap(); // (2)
            b.group(bossGroup, workerGroup)
             .channel(NioServerSocketChannel.class) // (3)
             .childHandler(new ChannelInitializer<SocketChannel>() { // (4)
                 @Override
                 public void initChannel(SocketChannel ch) throws Exception {
                     ch.pipeline().addLast(new DiscardServerHandler());
                 }
             })
             .option(ChannelOption.SO_BACKLOG, 128)          // (5)
             .childOption(ChannelOption.SO_KEEPALIVE, true); // (6)
    
            // Bind and start to accept incoming connections.
            ChannelFuture f = b.bind(port).sync(); // (7)
    
            // Wait until the server socket is closed.
            // In this example, this does not happen, but you can do that to gracefully
            // shut down your server.
            f.channel().closeFuture().sync();
        } finally {
            workerGroup.shutdownGracefully();
            bossGroup.shutdownGracefully();
        }
    }
    
    public static void main(String[] args) throws Exception {
        int port = 8080;
        if (args.length > 0) {
            port = Integer.parseInt(args[0]);
        }

        new DiscardServer(port).run();
    }
}
NioEventLoopGroup는 I/O 작업을 처리하는 다중 스레드 이벤트 루프입니다. Netty는 EventLoopGroup다양한 종류의 전송에 대해 다양한 구현을 제공합니다. 이 예제에서는 서버 측 애플리케이션을 구현하므로 두 가지 NioEventLoopGroup구현을 사용합니다. 'boss'라고 불리는 첫 번째 구현은 들어오는 연결을 수락합니다. 'worker'라고 불리는 두 번째 구현은 보스가 연결을 수락하고 수락된 연결을 워커에 등록하면 수락된 연결의 트래픽을 처리합니다. 사용되는 스레드 수와 생성된 Channel스레드에 매핑되는 방식은 구현에 따라 다르며 EventLoopGroup, 생성자를 통해 구성할 수도 있습니다.
ServerBootstrap서버를 설정하는 도우미 클래스입니다. 를 사용하여 서버를 직접 설정할 수 있습니다 Channel. 하지만 이 과정은 번거로우므로 대부분의 경우 직접 설정할 필요가 없습니다.
여기서는 들어오는 연결을 허용하기 위해 NioServerSocketChannel새로운 인스턴스를 생성하는 데 사용되는 클래스를 사용하도록 지정합니다 Channel.
여기에 지정된 핸들러는 항상 새로 승인된 에 의해 평가됩니다 Channel. 는 ChannelInitializer사용자가 새 를 구성하는 데 도움을 주기 위한 특수 핸들러입니다 . 네트워크 애플리케이션을 구현하기 위해 와 같은 핸들러를 추가하여 새 의 Channel를 구성하고자 할 가능성이 높습니다 . 애플리케이션이 복잡해짐에 따라 파이프라인에 더 많은 핸들러를 추가하고 이 익명 클래스를 최상위 클래스로 추출하게 될 가능성이 높습니다.ChannelPipelineChannelDiscardServerHandler
구현 에 따라 특정 매개변수를 설정할 수도 있습니다 Channel. TCP/IP 서버를 작성하고 있으므로, tcpNoDelay및 와 같은 소켓 옵션을 설정할 수 있습니다. 지원되는 s에 대한 개요는 해당 구현 keepAlive의 API 문서를 참조하세요 .ChannelOptionChannelConfigChannelOption
option()그리고 childOption()? 는 들어오는 연결을 허용하는 option()를 나타 냅니다 NioServerSocketChannel. 는 부모 에서 허용하는 s를 childOption()나타냅니다 . 이 경우에는 입니다 .ChannelServerChannelNioSocketChannel
이제 시작할 준비가 되었습니다. 이제 포트에 바인딩하고 서버를 시작하는 것만 남았습니다. 여기서는 8080머신에 있는 모든 NIC(네트워크 인터페이스 카드)의 포트에 바인딩합니다. 이제 서로 bind()다른 바인딩 주소를 사용하여 원하는 만큼 메서드를 호출할 수 있습니다.
축하합니다! Netty를 기반으로 첫 번째 서버를 완성했습니다.

수신된 데이터 살펴보기
첫 번째 서버를 작성했으니 이제 실제로 작동하는지 테스트해야 합니다. 가장 쉬운 방법은 telnet 명령을 사용하는 것입니다. 예를 들어, telnet localhost 8080명령줄에 다음과 같이 입력할 수 있습니다.

하지만 서버가 제대로 작동한다고 할 수 있을까요? 폐기 서버이기 때문에 제대로 알 수 없습니다. 아무런 응답도 받지 못할 것입니다. 서버가 정말 작동하는지 확인하기 위해, 수신된 메시지를 출력하도록 서버를 수정해 보겠습니다.

데이터가 수신될 때마다 해당 메서드가 호출된다는 것을 이미 알고 있습니다 . 메서드 channelRead()에 다음 코드를 추가해 보겠습니다 .channelRead()DiscardServerHandler

@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) {
    ByteBuf in = (ByteBuf) msg;
    try {
        while (in.isReadable()) { // (1)
            System.out.print((char) in.readByte());
            System.out.flush();
        }
    } finally {
        ReferenceCountUtil.release(msg); // (2)
    }
}
이 비효율적인 루프는 실제로 다음과 같이 단순화될 수 있습니다.System.out.println(in.toString(io.netty.util.CharsetUtil.US_ASCII))
혹은 in.release()여기서 할 수도 있습니다.
telnet 명령을 다시 실행하면 서버가 수신한 내용을 인쇄하는 것을 볼 수 있습니다.

폐기 서버의 전체 소스 코드는 io.netty.example.discard배포판 패키지에 들어 있습니다.

에코 서버 작성
지금까지 우리는 아무런 응답 없이 데이터를 소비해 왔습니다. 하지만 서버는 일반적으로 요청에 응답해야 합니다. ECHO수신된 데이터를 다시 전송하는 프로토콜을 구현하여 클라이언트에게 응답 메시지를 작성하는 방법을 알아보겠습니다.

이전 섹션에서 구현한 discard server와의 유일한 차이점은 수신된 데이터를 콘솔에 출력하는 대신 다시 전송한다는 것입니다. 따라서 메서드를 다시 수정하면 됩니다 channelRead().

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) {
        ctx.write(msg); // (1)
        ctx.flush(); // (2)
    }
객체 ChannelHandlerContext는 다양한 I/O 이벤트와 작업을 트리거할 수 있는 다양한 작업을 제공합니다. 여기서는 write(Object)수신된 메시지를 그대로 쓰기 위해 호출합니다. 예제와 달리 수신된 메시지를 해제하지 않았다는 점에 유의하세요 DISCARD. Netty가 메시지를 와이어에 쓸 때 자동으로 해제하기 때문입니다.
ctx.write(Object)메시지를 와이어에 기록하지 않습니다. 내부적으로 버퍼링된 후 와이어에 플러시됩니다 ctx.flush(). 또는 ctx.writeAndFlush(msg)간결성을 위해 호출할 수 있습니다.
telnet 명령을 다시 실행하면 서버가 사용자가 보낸 내용을 다시 보내는 것을 볼 수 있습니다.

에코 서버의 전체 소스 코드는 io.netty.example.echo배포판 패키지에 들어 있습니다.

타임 서버 작성
이 섹션에서 구현할 프로토콜은 TIME프로토콜입니다. 이 프로토콜은 요청을 수신하지 않고 32비트 정수를 포함하는 메시지를 전송하고, 전송 후 연결을 종료한다는 점에서 이전 예제와 다릅니다. 이 예제에서는 메시지를 생성하고 전송하는 방법과 완료 시 연결을 종료하는 방법을 알아봅니다.

수신된 데이터는 무시하고 연결이 설정되는 즉시 메시지를 전송하려고 하므로 channelRead()이번에는 메서드를 사용할 수 없습니다. 대신 channelActive()메서드를 재정의해야 합니다. 구현은 다음과 같습니다.

package io.netty.example.time;

public class TimeServerHandler extends ChannelInboundHandlerAdapter {

    @Override
    public void channelActive(final ChannelHandlerContext ctx) { // (1)
        final ByteBuf time = ctx.alloc().buffer(4); // (2)
        time.writeInt((int) (System.currentTimeMillis() / 1000L + 2208988800L));
        
        final ChannelFuture f = ctx.writeAndFlush(time); // (3)
        f.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) {
                assert f == future;
                ctx.close();
            }
        }); // (4)
    }
    
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        cause.printStackTrace();
        ctx.close();
    }
}
설명한 대로, channelActive()연결이 설정되고 트래픽을 생성할 준비가 되면 이 메서드가 호출됩니다. 이 메서드에서 현재 시간을 나타내는 32비트 정수를 작성해 보겠습니다.

새 메시지를 보내려면 메시지를 저장할 새 버퍼를 할당해야 합니다. 32비트 정수를 쓸 예정이므로 ByteBuf최소 4바이트 용량의 버퍼가 필요합니다. 다음을 통해 현재 ByteBufAllocator버퍼를 가져오고 ChannelHandlerContext.alloc()새 버퍼를 할당합니다.

평소처럼 우리는 구성된 메시지를 씁니다.

java.nio.ByteBuffer.flip()그런데 잠깐, 플립은 어디 있는 거지? NIO에서는 메시지를 보내기 전에 호출했었잖아 ? NIO에는 ByteBuf두 개의 포인터가 있어서 그런 메서드가 없어. 하나는 읽기 작업을 위한 것이고 다른 하나는 쓰기 작업을 위한 거야. 쓰기 인덱스는 무언가를 쓸 때 증가하는 반면, ByteBuf읽기 인덱스는 변하지 않아. 읽기 인덱스와 쓰기 인덱스는 각각 메시지의 시작과 끝을 나타낸다.

반면, NIO 버퍼는 flip 메서드를 호출하지 않고도 메시지 내용의 시작과 끝을 파악할 수 있는 명확한 방법을 제공하지 않습니다. 버퍼를 flip하는 것을 잊어버리면 아무것도 전송되지 않거나 잘못된 데이터가 전송되어 문제가 발생합니다. Netty에서는 각 연산 유형에 대해 서로 다른 포인터를 사용하기 때문에 이러한 오류가 발생하지 않습니다. 익숙해지면 flip out 없이 훨씬 수월하게 작업할 수 있다는 것을 알게 될 것입니다!

또 다른 주의점은 ChannelHandlerContext.write()(and writeAndFlush()) 메서드가 a 를 반환한다는 것입니다 ChannelFuture. A 는 ChannelFuture아직 수행되지 않은 I/O 작업을 나타냅니다. 즉, Netty에서는 모든 작업이 비동기적으로 실행되므로 요청된 작업이 아직 수행되지 않았을 수 있습니다. 예를 들어, 다음 코드는 메시지가 전송되기 전에 연결을 종료할 수 있습니다.

Channel ch = ...;
ch.writeAndFlush(message);
ch.close();
따라서 메서드에서 반환된 작업이 완료된 close()후 해당 메서드 를 호출해야 하며 , 쓰기 작업이 완료되면 리스너에게 알립니다. 연결이 즉시 닫히지 않을 수도 있으며, . 를 반환한다는 점에 유의하세요.ChannelFuturewrite()close()ChannelFuture

ChannelFutureListener그러면 쓰기 요청이 완료되면 어떻게 알림을 받을 수 있을까요? 반환된 에 를 추가하는 것만큼 간단합니다 ChannelFuture. 여기서는 작업이 완료되면 ChannelFutureListener를 닫는 새로운 익명 메서드를 만들었습니다.Channel

또는 미리 정의된 리스너를 사용하여 코드를 단순화할 수 있습니다.

f.addListener(ChannelFutureListener.CLOSE);
시간 서버가 예상대로 작동하는지 테스트하려면 다음 UNIX rdate명령을 사용할 수 있습니다.

$ rdate -o <port> -p <host>
여기서 <port>는 메서드에서 지정한 포트 번호 main()이며 <host>일반적으로 .입니다 localhost.

시간 클라이언트 작성
DISCARD서버 와 달리 ECHO, 프로토콜을 사용하려면 클라이언트가 필요합니다. TIME사람이 32비트 이진 데이터를 달력의 날짜로 변환할 수 없기 때문입니다. 이 섹션에서는 서버가 제대로 작동하는지 확인하고 Netty를 사용하여 클라이언트를 작성하는 방법을 알아봅니다.

Netty에서 서버와 클라이언트의 가장 크고 유일한 차이점은 서로 다른 구현 방식이 사용된다는 것입니다 Bootstrap. Channel다음 코드를 살펴보세요.

package io.netty.example.time;

public class TimeClient {
    public static void main(String[] args) throws Exception {
        String host = args[0];
        int port = Integer.parseInt(args[1]);
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        
        try {
            Bootstrap b = new Bootstrap(); // (1)
            b.group(workerGroup); // (2)
            b.channel(NioSocketChannel.class); // (3)
            b.option(ChannelOption.SO_KEEPALIVE, true); // (4)
            b.handler(new ChannelInitializer<SocketChannel>() {
                @Override
                public void initChannel(SocketChannel ch) throws Exception {
                    ch.pipeline().addLast(new TimeClientHandler());
                }
            });
            
            // Start the client.
            ChannelFuture f = b.connect(host, port).sync(); // (5)

            // Wait until the connection is closed.
            f.channel().closeFuture().sync();
        } finally {
            workerGroup.shutdownGracefully();
        }
    }
}
BootstrapServerBootstrap는 클라이언트 측이나 연결 없는 채널과 같은 비서버 채널을 위한 것이라는 점을 제외하면 와 비슷합니다 .
하나만 지정하면 EventLoopGroup보스 그룹과 워커 그룹 모두로 사용됩니다. 하지만 보스 워커는 클라이언트 측에서는 사용되지 않습니다.
대신에 NioServerSocketChannel, NioSocketChannel클라이언트 측을 생성하는 데 사용됩니다 Channel.
클라이언트 측에 부모가 없기 때문에 childOption()여기서는 와 달리 를 사용하지 않는다는 점에 유의하세요 .ServerBootstrapSocketChannel
connect()메서드 대신 메서드를 호출해야 합니다 bind().
보시다시피 서버 측 코드와 크게 다르지 않습니다. ChannelHandler구현은 어떻게 될까요? 서버로부터 32비트 정수를 받아 사람이 읽을 수 있는 형식으로 변환하고, 변환된 시간을 출력한 후 연결을 종료합니다.

package io.netty.example.time;

import java.util.Date;

import io.netty.buffer.ByteBuf;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;

public class TimeClientHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) {
        ByteBuf m = (ByteBuf) msg; // (1)
        try {
            long currentTimeMillis = (m.readUnsignedInt() - 2208988800L) * 1000L;
            System.out.println(new Date(currentTimeMillis));
            ctx.close();
        } finally {
            m.release();
        }
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        cause.printStackTrace();
        ctx.close();
    }
}
TCP/IP에서 Netty는 피어로부터 전송된 데이터를 읽습니다 ByteBuf.
매우 간단해 보이고 서버 측 예제와 크게 다르지 않습니다. 하지만 이 핸들러는 때때로 . 를 발생시켜 작동하지 않을 수 있습니다 IndexOutOfBoundsException. 다음 섹션에서 이런 현상이 발생하는 이유를 살펴보겠습니다.

스트림 기반 전송 처리
소켓 버퍼의 작은 경고 하나
TCP/IP와 같은 스트림 기반 전송에서는 수신된 데이터가 소켓 수신 버퍼에 저장됩니다. 하지만 스트림 기반 전송의 버퍼는 패킷 큐가 아니라 바이트 큐입니다. 즉, 두 개의 메시지를 두 개의 독립된 패킷으로 전송하더라도 운영 체제는 이를 두 개의 메시지로 처리하지 않고 바이트 묶음으로 처리합니다. 따라서 사용자가 읽는 내용이 원격 피어가 작성한 내용과 정확히 일치한다는 보장은 없습니다. 예를 들어, 운영 체제의 TCP/IP 스택이 세 개의 패킷을 수신했다고 가정해 보겠습니다.

전송된 대로 수신된 3개의 패킷

스트림 기반 프로토콜의 이러한 일반적인 속성 때문에 애플리케이션에서 다음과 같은 조각난 형태로 읽을 가능성이 높습니다.

3개의 패킷이 분할되어 4개의 버퍼로 병합되었습니다.

따라서 서버 측이든 클라이언트 측이든 수신 측은 수신된 데이터를 애플리케이션 로직이 쉽게 이해할 수 있는 하나 이상의 의미 있는 프레임으로 분할해야 합니다. 위 예의 경우, 수신된 데이터는 다음과 같은 프레임으로 구성되어야 합니다.

4개의 버퍼가 3개로 조각 모음됨

첫 번째 솔루션
이제 클라이언트 예제로 돌아가 보겠습니다 TIME. 여기서도 같은 문제가 있습니다. 32비트 정수는 매우 적은 양의 데이터이므로 자주 단편화될 가능성이 낮습니다. 하지만 문제는 단편화될 수 있으며, 트래픽이 증가함에 따라 단편화 가능성이 높아진다는 것입니다.

가장 간단한 해결책은 내부 누적 버퍼를 생성하고 4바이트가 모두 내부 버퍼에 수신될 때까지 기다리는 것입니다. 다음은 TimeClientHandler이 문제를 해결하는 수정된 구현입니다.

package io.netty.example.time;

public class TimeClientHandler extends ChannelInboundHandlerAdapter {
    private ByteBuf buf;
    
    @Override
    public void handlerAdded(ChannelHandlerContext ctx) {
        buf = ctx.alloc().buffer(4); // (1)
    }
    
    @Override
    public void handlerRemoved(ChannelHandlerContext ctx) {
        buf.release(); // (1)
        buf = null;
    }
    
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) {
        ByteBuf m = (ByteBuf) msg;
        buf.writeBytes(m); // (2)
        m.release();
        
        if (buf.readableBytes() >= 4) { // (3)
            long currentTimeMillis = (buf.readUnsignedInt() - 2208988800L) * 1000L;
            System.out.println(new Date(currentTimeMillis));
            ctx.close();
        }
    }
    
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        cause.printStackTrace();
        ctx.close();
    }
}
A ChannelHandler에는 두 가지 라이프사이클 리스너 메서드가 있습니다: handlerAdded()와 handlerRemoved(). 장시간 차단되지 않는 한 임의의 초기화(해제) 작업을 수행할 수 있습니다.
첫째, 수신된 모든 데이터를 누적해야 합니다 buf.
그런 다음, 핸들러는 buf충분한 데이터(이 예에서는 4바이트)가 있는지 확인하고 실제 비즈니스 로직으로 진행해야 합니다. 그렇지 않으면 Netty는 channelRead()더 많은 데이터가 도착할 때 메서드를 다시 호출하여 결국 4바이트를 모두 누적합니다.
두 번째 해결책
첫 번째 해결책으로 클라이언트 문제는 해결되었지만 TIME, 수정된 핸들러는 그다지 깔끔해 보이지 않습니다. 가변 길이 필드와 같이 여러 필드로 구성된 더 복잡한 프로토콜을 상상해 보세요. ChannelInboundHandler구현 방식은 매우 빠르게 유지 관리가 불가능해질 것입니다.

아시다시피, ChannelHandler에 여러 개를 추가할 수 있으므로 ChannelPipeline하나의 모놀리식 핸들러를 여러 개의 모듈식 핸들러로 분할하여 ChannelHandler애플리케이션의 복잡성을 줄일 수 있습니다. 예를 들어, 다음과 같이 TimeClientHandler두 개의 핸들러로 분할할 수 있습니다.

TimeDecoder이는 단편화 문제를 다루고 있습니다.
TimeClientHandler. 의 초기 간단한 버전 .
다행히도 Netty는 첫 번째 클래스를 바로 작성할 수 있도록 도와주는 확장 가능한 클래스를 제공합니다.

package io.netty.example.time;

public class TimeDecoder extends ByteToMessageDecoder { // (1)
    @Override
    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) { // (2)
        if (in.readableBytes() < 4) {
            return; // (3)
        }
        
        out.add(in.readBytes(4)); // (4)
    }
}
ByteToMessageDecoderChannelInboundHandler단편화 문제를 쉽게 처리할 수 있는 구현입니다 .
ByteToMessageDecoderdecode()새로운 데이터가 수신될 때마다 내부적으로 유지 관리되는 누적 버퍼로 메서드를 호출합니다 .
decode()out누적 버퍼에 충분한 데이터가 없으면 아무것도 추가하지 않기로 결정할 수 있습니다 . 더 많은 데이터를 받으면 다시 ByteToMessageDecoder호출합니다 .decode()
decode()에 객체를 추가하면 디코더 out가 메시지를 성공적으로 디코딩했음을 의미합니다. ByteToMessageDecoder누적 버퍼에서 읽은 부분을 삭제합니다. 여러 메시지를 디코딩할 필요는 없습니다. 에 아무것도 추가되지 않을 때까지 메서드를 ByteToMessageDecoder계속 호출합니다 .decode()out
이제 삽입할 또 다른 핸들러가 있으므로 , 다음에서 구현을 ChannelPipeline수정해야 합니다 .ChannelInitializerTimeClient

b.handler(new ChannelInitializer<SocketChannel>() {
    @Override
    public void initChannel(SocketChannel ch) throws Exception {
        ch.pipeline().addLast(new TimeDecoder(), new TimeClientHandler());
    }
});
모험심이 많으시다면 디코더를 더욱 간소화하는 를 사용해 보세요 ReplayingDecoder. 하지만 더 자세한 내용은 API 참조를 참조해야 합니다.

public class TimeDecoder extends ReplayingDecoder<Void> {
    @Override
    protected void decode(
            ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {
        out.add(in.readBytes(4));
    }
}
또한, Netty는 대부분의 프로토콜을 매우 쉽게 구현할 수 있도록 하는 기본 제공 디코더를 제공하며, 유지 관리가 어려운 모놀리식 핸들러 구현으로 끝나는 것을 방지합니다. 더 자세한 예제는 다음 패키지를 참조하세요.

io.netty.example.factorial이진 프로토콜의 경우
io.netty.example.telnet텍스트 라인 기반 프로토콜의 경우.
POJO로 말하기 대신ByteBuf
지금까지 살펴본 모든 예제는 프로토콜 메시지의 기본 데이터 구조로 .을 사용했습니다 . 이 섹션에서는 . 대신 POJO를 사용하도록 프로토콜 클라이언트 및 서버 예제를 ByteBuf개선해 보겠습니다 .TIMEByteBuf

s 에서 POJO를 사용하는 이점은 명확합니다. 핸들러에서 ChannelHandler정보를 추출하는 코드를 분리함으로써 핸들러의 유지 관리 및 재사용성이 향상됩니다 . 클라이언트와 서버 예제에서는 32비트 정수 하나만 읽었으며, 직접 사용하는 데 큰 문제는 없습니다 . 하지만 실제 프로토콜을 구현할 때는 이러한 분리가 필요하다는 것을 알게 될 것입니다.ByteBufTIMEByteBuf

먼저, 라는 새로운 유형을 정의해 보겠습니다 UnixTime.

package io.netty.example.time;

import java.util.Date;

public class UnixTime {

    private final long value;
    
    public UnixTime() {
        this(System.currentTimeMillis() / 1000L + 2208988800L);
    }
    
    public UnixTime(long value) {
        this.value = value;
    }
        
    public long value() {
        return value;
    }
        
    @Override
    public String toString() {
        return new Date((value() - 2208988800L) * 1000L).toString();
    }
}
이제 a 대신 a를 TimeDecoder생성하도록 수정할 수 있습니다 .UnixTimeByteBuf

@Override
protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {
    if (in.readableBytes() < 4) {
        return;
    }

    out.add(new UnixTime(in.readUnsignedInt()));
}
업데이트된 디코더에서는 더 이상 다음을 TimeClientHandler사용할 수 없습니다 ByteBuf.

@Override
public void channelRead(ChannelHandlerContext ctx, Object msg) {
    UnixTime m = (UnixTime) msg;
    System.out.println(m);
    ctx.close();
}
훨씬 간단하고 우아하죠? 서버 측에도 같은 기술을 적용할 수 있습니다. TimeServerHandler이번에는 첫 번째 방법을 업데이트해 보겠습니다.

@Override
public void channelActive(ChannelHandlerContext ctx) {
    ChannelFuture f = ctx.writeAndFlush(new UnixTime());
    f.addListener(ChannelFutureListener.CLOSE);
}
이제 유일하게 부족한 부분은 인코더입니다. 인코더는 ChannelOutboundHandlera를 UnixTimea로 다시 변환하는 구현체입니다 ByteBuf. 메시지를 인코딩할 때 패킷 조각화 및 조립을 처리할 필요가 없기 때문에 디코더를 작성하는 것보다 훨씬 간단합니다.

package io.netty.example.time;

public class TimeEncoder extends ChannelOutboundHandlerAdapter {
    @Override
    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) {
        UnixTime m = (UnixTime) msg;
        ByteBuf encoded = ctx.alloc().buffer(4);
        encoded.writeInt((int)m.value());
        ctx.write(encoded, promise); // (1)
    }
}
이 한 줄에는 꽤 중요한 내용이 담겨 있습니다.

첫째, 인코딩된 데이터가 실제로 와이어에 쓰여질 때 Netty가 성공 또는 실패로 표시하도록 원본을 ChannelPromise그대로 전달합니다.

둘째, 호출하지 않았습니다 . 해당 작업을 재정의하기 위한 ctx.flush()별도의 핸들러 메서드가 있습니다 .void flush(ChannelHandlerContext ctx)flush()

더욱 단순화하려면 다음을 활용할 수 있습니다 MessageToByteEncoder.

public class TimeEncoder extends MessageToByteEncoder<UnixTime> {
    @Override
    protected void encode(ChannelHandlerContext ctx, UnixTime msg, ByteBuf out) {
        out.writeInt((int)msg.value());
    }
}
마지막 으로 남은 작업은 서버 측에서 TimeEncoder에 를 삽입하는 것이며 , 이는 사소한 작업으로 남겨집니다.ChannelPipelineTimeServerHandler

애플리케이션 종료
EventLoopGroupNetty 애플리케이션을 종료하는 것은 일반적으로 를 통해 생성한 모든 s를 종료하는 것만큼 간단합니다 . s가 완전히 종료되고 그룹에 속한 모든 s가 닫혔음을 알려주는 를 shutdownGracefully()반환합니다 .FutureEventLoopGroupChannel

요약
이 장에서는 Netty를 빠르게 살펴보고 Netty를 기반으로 완벽하게 작동하는 네트워크 애플리케이션을 작성하는 방법을 시연했습니다.

다음 장에서는 Netty에 대한 자세한 정보를 제공합니다. io.netty.example패키지에 포함된 Netty 예제도 살펴보시기 바랍니다.

또한 커뮤니티는 항상 여러분의 질문과 아이디어를 기다리고 있으며, 이를 바탕으로 Netty와 관련 문서를 개선해 나갈 것입니다.